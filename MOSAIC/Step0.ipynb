{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7314c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as n\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b05cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_microbiomeHD(address_directory, output_root = False, id = 'Sam_id', covar_l = []):\n",
    "    ### note that due to the complexity of metadata, the current microbiomeHD loading does \n",
    "    ### not take into account the covariates other than batches and diseaseStates\n",
    "    ### so default vars_use will just be Dataset\n",
    "    subdir_names = os.listdir(address_directory)\n",
    "    subdir_names = [result for result in subdir_names if \"results\" in result]\n",
    "    count_data_l = []\n",
    "    intersect_taxa = []\n",
    "    metadata_l = []\n",
    "    crc_baxter_ids = None \n",
    "    \n",
    "    for subdir in subdir_names:\n",
    "        ## 1. get the otu table part\n",
    "        # get the otu file\n",
    "        subdir_path = address_directory + '/' + subdir\n",
    "        current_RDP_names = os.listdir(subdir_path + '/RDP')\n",
    "        current_dbotu_count_data_path = [result for result in current_RDP_names if \"100.denovo.rdp_assigned\" in result][0]\n",
    "        current_dbotu_count_data = pd.read_csv(subdir_path+'/RDP/'+current_dbotu_count_data_path, delimiter='\\t', index_col=0)\n",
    "        current_taxa = list(current_dbotu_count_data.index)\n",
    "        # set index with genus level\n",
    "        current_taxa = [\";\".join(taxa.split(';')[:-2]) for taxa in current_taxa]\n",
    "        print(current_dbotu_count_data.shape)\n",
    "        current_dbotu_count_data.index = current_taxa\n",
    "        print(current_dbotu_count_data.shape)\n",
    "        # remove duplicate indexes by summing up rows with the same index\n",
    "        current_dbotu_count_data = current_dbotu_count_data.groupby(level=0).sum()\n",
    "        \n",
    "        # save dataframe and feature list\n",
    "        count_data_l.append(current_dbotu_count_data)\n",
    "        \n",
    "        if intersect_taxa == []:\n",
    "            intersect_taxa = current_taxa\n",
    "        else:\n",
    "            intersect_taxa = list(set(intersect_taxa).intersection(current_taxa))\n",
    "        \n",
    "        # Output how many taxonomic units in each OTU table are contained in intersect_taxa\n",
    "        num_intersecting_taxa = sum(current_dbotu_count_data.index.isin(intersect_taxa))\n",
    "        print(f\"{subdir} OTU table contains {num_intersecting_taxa} genus-level taxa that are in intersect_taxa.\")\n",
    "\n",
    "\n",
    "        ## 2. get the metadata\n",
    "        # get the metadata file\n",
    "        current_files_names = os.listdir(subdir_path)\n",
    "        current_metadata_path = subdir_path + '/' + [result for result in current_files_names if \"metadata\" in result][0]\n",
    "        print(\"metadata path\", current_metadata_path)\n",
    "        current_metadata = pd.read_csv(current_metadata_path, delimiter='\\t', index_col=0, encoding='ISO-8859-1')['DiseaseState'].to_frame()\n",
    "        current_metadata['Dataset'] = [\"_\".join(subdir.split(\"_\")[:-1])]*current_metadata.shape[0]\n",
    "        print(current_metadata.shape)\n",
    "\n",
    "        # save crc_baxter_results Sample IDs\n",
    "        if 'crc_baxter_results' in subdir:\n",
    "            crc_baxter_ids = set(current_metadata.index)\n",
    "\n",
    "        # get covariates if exists\n",
    "        if covar_l != []:\n",
    "            current_covars = pd.read_csv(current_metadata_path, delimiter='\\t', index_col=0, encoding='ISO-8859-1')[covar_l]\n",
    "            current_metadata = pd.concat([current_metadata, current_covars], axis=1)\n",
    "        metadata_l.append(current_metadata)\n",
    "\n",
    "    # intersect count data list\n",
    "    intersect_count_data_l = [count_data[count_data.index.isin(intersect_taxa)] for count_data in count_data_l]\n",
    "\n",
    "    # generate results\n",
    "    combined_countdf = pd.concat(intersect_count_data_l, axis=1)\n",
    "    combined_countdf = combined_countdf.dropna().T\n",
    "    combined_metadata = pd.concat(metadata_l)\n",
    "    combined_metadata[id] = list(combined_metadata.index)  # the default IDCol for microbiomeHD will be Sam_id\n",
    "\n",
    "    data_mat, meta_data = preprocess(combined_countdf, combined_metadata, id, covar_l)\n",
    "\n",
    "    # ensure that the sample ids are correctly aligned in metadata and count_table\n",
    "    data_mat_ids = set(data_mat.index)\n",
    "    meta_data_ids = set(meta_data.index)\n",
    "    intersection_ids = data_mat_ids.intersection(meta_data_ids)\n",
    "\n",
    "    # check if Sample IDs of crc_baxter_results in intersection_ids \n",
    "    if crc_baxter_ids is not None:\n",
    "        crc_baxter_intersection = crc_baxter_ids.intersection(intersection_ids)\n",
    "        if len(crc_baxter_intersection) == 0:\n",
    "            print(\"Warning: No Sample IDs from crc_baxter_results were retained after merging.\")\n",
    "        else:\n",
    "            print(f\"Sample IDs from crc_baxter_results retained: {len(crc_baxter_intersection)} out of {len(crc_baxter_ids)}\")\n",
    "\n",
    "    # drop rows where indexes are not overlapping\n",
    "    data_mat_non_intersecting = [id for id in data_mat_ids if id not in intersection_ids]\n",
    "    data_mat = data_mat.drop(data_mat_non_intersecting)\n",
    "    meta_data_non_intersecting = [id for id in meta_data_ids if id not in intersection_ids]\n",
    "    meta_data = meta_data.drop(meta_data_non_intersecting)\n",
    "    data_mat = data_mat.reindex(intersection_ids)\n",
    "    meta_data = meta_data.reindex(intersection_ids)\n",
    "\n",
    "    # save stuff if needed\n",
    "    if output_root != False:\n",
    "        data_mat.to_csv(output_root + \"_count_data.csv\")\n",
    "        meta_data.to_csv(output_root + \"_meta_data.csv\", index=False)\n",
    "    return data_mat, meta_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7536c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_mat, meta_data, IDCol, covar_l = []):\n",
    "    initial_crc_baxter_count = meta_data[meta_data['Dataset'] == 'crc_baxter'].shape[0]\n",
    "    print(f\"Initial crc_baxter_results samples: {initial_crc_baxter_count}\")\n",
    "\n",
    "    # Step 1: Remove samples with all zeros\n",
    "    data_mat = data_mat.loc[~(data_mat==0).all(axis=1)]\n",
    "    kept_samples = data_mat.index\n",
    "    meta_data = meta_data[meta_data[IDCol].isin(kept_samples)]\n",
    "    \n",
    "    crc_baxter_after_step1 = meta_data[meta_data['Dataset'] == 'crc_baxter'].shape[0]\n",
    "    print(f\"After Step 1 (remove all-zero samples): {crc_baxter_after_step1} crc_baxter_results samples retained\")\n",
    "\n",
    "    # Step 2: Remove features with all zeros\n",
    "    col_names = list(data_mat.columns)\n",
    "    col_sums = data_mat.sum(axis = 1)\n",
    "    removable_feature_names = [col_names[index] for index, col_sum in enumerate(col_sums) if col_sum==0]\n",
    "    data_mat.drop(removable_feature_names, axis=1, inplace=True)\n",
    "    \n",
    "    # Step 3: Remove samples with missing values for each covariate\n",
    "    for covar in covar_l:\n",
    "        meta_data = meta_data[meta_data[covar].notna()]\n",
    "    \n",
    "    crc_baxter_after_step3 = meta_data[meta_data['Dataset'] == 'crc_baxter'].shape[0]\n",
    "    print(f\"After Step 3 (remove samples with missing covariates): {crc_baxter_after_step3} crc_baxter_results samples retained\")\n",
    "\n",
    "    # Step 4: Ensure data_mat and meta_data alignment\n",
    "    data_mat = data_mat.loc[meta_data[IDCol]]\n",
    "    \n",
    "    # Step 5: Final feature removal with all zeros\n",
    "    col_names = list(data_mat.columns)\n",
    "    col_sums = data_mat.sum(axis = 1)\n",
    "    removable_feature_names = [col_names[index] for index, col_sum in enumerate(col_sums) if col_sum==0]\n",
    "    data_mat.drop(removable_feature_names, axis=1, inplace=True)\n",
    "\n",
    "    crc_baxter_final = meta_data[meta_data['Dataset'] == 'crc_baxter'].shape[0]\n",
    "    print(f\"Final crc_baxter_results samples retained: {crc_baxter_final}\")\n",
    "\n",
    "    return data_mat, meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "623b0037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1617, 43)\n",
      "(1617, 43)\n",
      "crc_xiang_results OTU table contains 136 genus-level taxa that are in intersect_taxa.\n",
      "metadata path /Users/hehehe/Desktop/CRC//microbiomeHD/crc_xiang_results/crc_xiang.metadata.txt\n",
      "(43, 2)\n",
      "(837, 102)\n",
      "(837, 102)\n",
      "crc_zhao_results OTU table contains 80 genus-level taxa that are in intersect_taxa.\n",
      "metadata path /Users/hehehe/Desktop/CRC//microbiomeHD/crc_zhao_results/crc_zhao.metadata.txt\n",
      "(102, 2)\n",
      "(82665, 129)\n",
      "(82665, 129)\n",
      "crc_zeller_results OTU table contains 78 genus-level taxa that are in intersect_taxa.\n",
      "metadata path /Users/hehehe/Desktop/CRC//microbiomeHD/crc_zeller_results/crc_zeller.metadata.txt\n",
      "(156, 2)\n",
      "(122510, 490)\n",
      "(122510, 490)\n",
      "crc_baxter_results OTU table contains 76 genus-level taxa that are in intersect_taxa.\n",
      "metadata path /Users/hehehe/Desktop/CRC//microbiomeHD/crc_baxter_results/crc_baxter.metadata.txt\n",
      "(490, 2)\n",
      "(70846, 92)\n",
      "(70846, 92)\n",
      "crc_zackular_results OTU table contains 76 genus-level taxa that are in intersect_taxa.\n",
      "metadata path /Users/hehehe/Desktop/CRC//microbiomeHD/crc_zackular_results/crc_zackular.metadata.txt\n",
      "(90, 2)\n",
      "Initial crc_baxter_results samples: 490\n",
      "After Step 1 (remove all-zero samples): 0 crc_baxter_results samples retained\n",
      "After Step 3 (remove samples with missing covariates): 0 crc_baxter_results samples retained\n",
      "Final crc_baxter_results samples retained: 0\n",
      "Warning: No Sample IDs from crc_baxter_results were retained after merging.\n"
     ]
    }
   ],
   "source": [
    "overall_path = '/Users/hehehe/Desktop/CRC/'\n",
    "output_dir_path = overall_path+'/MOSAIC/microbiomeHD Merge/intermediate_microbiomeHD'\n",
    "address_directory = overall_path+'/microbiomeHD'\n",
    "data_mat, meta_data = load_data_microbiomeHD(address_directory, output_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc66e88",
   "metadata": {},
   "source": [
    "## crc_baxter_results are all removed in 'preprocess', \n",
    "## and in this Step0 we use 'load_data_microbiomeHD' which contains 'preprocess', \n",
    "## so there's no crc_baxter_results in 'microbiomeHD Merge'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
